#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡å¼é€å¥éŸ³é¢‘ç”Ÿæˆå™¨
æŒ‰å¥å­é€ä¸ªç”ŸæˆéŸ³é¢‘ï¼Œæ”¯æŒè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
"""

import os
import sys
import json
import hashlib
import argparse
import time
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from tts.infer_cli import MegaTTS3DiTInfer
from tts.utils.text_utils.split_text import chunk_text_chinesev2, chunk_text_english
from langdetect import detect as classify_language
import torch

class IncrementalTTSGenerator:
    def __init__(self, reference_wav, reference_npy=None, device=None):
        self.reference_wav = reference_wav
        self.reference_npy = reference_npy or reference_wav.replace('.wav', '.npy')
        self.device = device or ("mps" if torch.backends.mps.is_available() else "cpu")

        # åˆå§‹åŒ–æ¨¡å‹
        print(f"ğŸ”„ åˆå§‹åŒ–æ¨¡å‹ï¼Œè®¾å¤‡: {self.device}")
        self.infer_pipe = MegaTTS3DiTInfer(device=self.device)

        # åŠ è½½å‚è€ƒéŸ³é¢‘
        print("ğŸ”„ åŠ è½½å‚è€ƒéŸ³é¢‘...")
        with open(self.reference_wav, 'rb') as file:
            file_content = file.read()
        self.resource_context = self.infer_pipe.preprocess(file_content, latent_file=self.reference_npy)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # å¯¼å…¥å¿…è¦çš„å‡½æ•°
        from tts.frontend_function import g2p, dur_pred, prepare_inputs_for_dit
        from tts.utils.audio_utils.io import to_wav_bytes
        from tn.chinese.normalizer import Normalizer as ZhNormalizer
        from tn.english.normalizer import Normalizer as EnNormalizer
        import pyloudnorm as pyln
        import numpy as np

        self.g2p = g2p
        self.dur_pred = dur_pred
        self.prepare_inputs_for_dit = prepare_inputs_for_dit
        self.to_wav_bytes = to_wav_bytes
        self.zh_normalizer = ZhNormalizer()
        self.en_normalizer = EnNormalizer()
        self.pyln = pyln
        self.np = np

    def generate_filename(self, text, index=None):
        """ç”ŸæˆéŸ³é¢‘æ–‡ä»¶åï¼Œä½¿ç”¨å®Œæ•´æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰"""
        import re

        # å»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡
        # ä¿ç•™ç©ºæ ¼ç”¨äºåç»­æ›¿æ¢
        safe_text = re.sub(r'[^\w\s]', '', text)
        # å»é™¤å¤šä½™ç©ºæ ¼
        safe_text = re.sub(r'\s+', '_', safe_text.strip())

        # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­ä½†ä¿ç•™å®Œæ•´æ€§
        max_length = 200  # æ–‡ä»¶åæœ€å¤§é•¿åº¦
        if len(safe_text) > max_length:
            safe_text = safe_text[:max_length]

        if index is not None:
            filename = f"{index:03d}_{safe_text}.wav"
        else:
            filename = f"{safe_text}.wav"

        return filename

    def generate_single_sentence(self, text, seg_i, total_segs, time_step, p_w, t_w, dur_disturb=0.1, dur_alpha=1.0):
        """ç”Ÿæˆå•ä¸ªå¥å­çš„éŸ³é¢‘ï¼ˆåº•å±‚å®ç°ï¼Œä¸è°ƒç”¨forwardï¼‰"""
        device = self.device

        # ä» resource_context è·å–å‚è€ƒæ•°æ®
        ph_ref = self.resource_context['ph_ref'].to(device)
        tone_ref = self.resource_context['tone_ref'].to(device)
        mel2ph_ref = self.resource_context['mel2ph_ref'].to(device)
        vae_latent = self.resource_context['vae_latent'].to(device)
        ctx_dur_tokens = self.resource_context['ctx_dur_tokens'].to(device)
        incremental_state_dur_prompt = self.resource_context['incremental_state_dur_prompt']

        with torch.inference_mode():
            # G2P
            ph_pred, tone_pred = self.g2p(self.infer_pipe, text)

            # Duration Prediction
            mel2ph_pred = self.dur_pred(
                self.infer_pipe, ctx_dur_tokens, incremental_state_dur_prompt,
                ph_pred, tone_pred, seg_i, dur_disturb, dur_alpha,
                is_first=(seg_i==0), is_final=(seg_i==total_segs-1)
            )

            # Prepare inputs
            inputs = self.prepare_inputs_for_dit(
                self.infer_pipe, mel2ph_ref, mel2ph_pred,
                ph_ref, tone_ref, ph_pred, tone_pred, vae_latent
            )

            # DiT inference
            with torch.cuda.amp.autocast(dtype=self.infer_pipe.precision, enabled=True):
                x = self.infer_pipe.dit.inference(inputs, timesteps=time_step, seq_cfg_w=[p_w, t_w]).float()

            # WavVAE decode
            x[:, :vae_latent.size(1)] = vae_latent
            wav_pred = self.infer_pipe.wavvae.decode(x)[0,0].to(torch.float32)

            # Post-processing
            wav_pred = wav_pred[vae_latent.size(1)*self.infer_pipe.vae_stride*self.infer_pipe.hop_size:].cpu().numpy()

            # Normalize loudness
            meter = self.pyln.Meter(self.infer_pipe.sr)
            loudness_pred = self.infer_pipe.loudness_meter.integrated_loudness(wav_pred.astype(float))
            wav_pred = self.pyln.normalize.loudness(wav_pred, loudness_pred, self.infer_pipe.loudness_prompt)
            if self.np.abs(wav_pred).max() >= 1:
                wav_pred = wav_pred / self.np.abs(wav_pred).max() * 0.95

            return wav_pred

    def generate_single_audio(self, text, output_dir, index=None, total_segs=1, **kwargs):
        """ç”Ÿæˆå•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å
        filename = self.generate_filename(text, index)
        output_path = os.path.join(output_dir, filename)

        try:
            print(f"   ğŸ“ æ–‡æœ¬: {text[:50]}{'...' if len(text) > 50 else ''}")

            # ç”ŸæˆéŸ³é¢‘
            start_time = time.time()
            seg_i = (index - 1) if index else 0
            wav_pred = self.generate_single_sentence(
                text, seg_i, total_segs,
                kwargs.get('time_step', 16),
                kwargs.get('p_w', 1.6),
                kwargs.get('t_w', 2.5),
                kwargs.get('dur_disturb', 0.1),
                kwargs.get('dur_alpha', 1.0)
            )

            # è½¬æ¢ä¸º wav bytes å¹¶ä¿å­˜
            wav_bytes = self.to_wav_bytes(wav_pred, self.infer_pipe.sr)
            from tts.utils.audio_utils.io import save_wav
            save_wav(wav_bytes, output_path)

            generation_time = time.time() - start_time

            print(f"   âœ… å®Œæˆ - è€—æ—¶: {generation_time:.1f}ç§’")

            return {
                'status': 'success',
                'text': text,
                'output_path': output_path,
                'filename': filename,
                'index': index,
                'generation_time': generation_time
            }

        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'status': 'failed',
                'text': text,
                'output_path': None,
                'filename': filename,
                'index': index,
                'error': str(e)
            }

    def normalize_text_for_matching(self, text):
        """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºåŒ¹é…ï¼šå»é™¤æ‰€æœ‰æ ‡ç‚¹å’Œç©ºæ ¼"""
        import re
        return re.sub(r'[^\w]', '', text)

    def load_existing_report(self, output_dir):
        """åŠ è½½å·²æœ‰çš„ç”ŸæˆæŠ¥å‘Š"""
        report_path = os.path.join(output_dir, 'generation_report.json')
        if os.path.exists(report_path):
            try:
                with open(report_path, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                return report
            except Exception as e:
                print(f"âš ï¸  è¯»å–æŠ¥å‘Šå¤±è´¥: {e}")
        return None

    def build_existing_sentences_map(self, existing_report):
        """æ„å»ºå·²å¤„ç†å¥å­çš„æ˜ å°„è¡¨ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ–‡æœ¬ä½œä¸ºkeyï¼‰"""
        existing_sentences = {}
        if existing_report:
            for result in existing_report.get('results', []):
                if result['status'] in ['success', 'skipped']:
                    # æ ‡å‡†åŒ–æ–‡æœ¬ï¼šå»é™¤æ‰€æœ‰æ ‡ç‚¹å’Œç©ºæ ¼ï¼Œç”¨äºåŒ¹é…
                    normalized_text = self.normalize_text_for_matching(result['text'])
                    existing_sentences[normalized_text] = result
        return existing_sentences

    def process_text_file(self, text_file, output_dir, force_regenerate=False, **kwargs):
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘"""
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        with open(text_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()

        # æ£€æµ‹è¯­è¨€å¹¶åˆ†å¥
        language_type = classify_language(content)

        if language_type == 'en':
            print("ğŸŒ æ£€æµ‹åˆ°è‹±æ–‡æ–‡æœ¬")
            content = self.en_normalizer.normalize(content)
            text_segs = chunk_text_english(content, max_chars=130)
        else:
            print("ğŸŒ æ£€æµ‹åˆ°ä¸­æ–‡æ–‡æœ¬")
            content = self.zh_normalizer.normalize(content)
            text_segs = chunk_text_chinesev2(content, limit=60)

        print(f"ğŸ“ æ–‡æœ¬å·²åˆ†å¥ï¼Œå…± {len(text_segs)} å¥")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # å¤„ç†æ¯ä¸ªå¥å­
        results = []
        for i, sentence in enumerate(text_segs):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(text_segs)} å¥")

            # ç”Ÿæˆæ–‡ä»¶åï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            filename = self.generate_filename(sentence, i+1)
            output_path = os.path.join(output_dir, filename)

            if os.path.exists(output_path) and not force_regenerate:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
                print(f"   ğŸ“ æ–‡æœ¬: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
                results.append({
                    'status': 'skipped',
                    'text': sentence,
                    'output_path': output_path,
                    'filename': filename,
                    'index': i+1
                })
                continue

            result = self.generate_single_audio(
                sentence,
                output_dir,
                index=i+1,
                total_segs=len(text_segs),
                **kwargs
            )
            results.append(result)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(results, output_dir, text_file)

        return results

    def process_direct_text(self, input_text, output_dir, force_regenerate=False, **kwargs):
        """å¤„ç†ç›´æ¥è¾“å…¥çš„æ–‡æœ¬ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘"""
        # æ£€æµ‹è¯­è¨€å¹¶åˆ†å¥
        language_type = classify_language(input_text)

        if language_type == 'en':
            print("ğŸŒ æ£€æµ‹åˆ°è‹±æ–‡æ–‡æœ¬")
            input_text = self.en_normalizer.normalize(input_text)
            text_segs = chunk_text_english(input_text, max_chars=130)
        else:
            print("ğŸŒ æ£€æµ‹åˆ°ä¸­æ–‡æ–‡æœ¬")
            input_text = self.zh_normalizer.normalize(input_text)
            text_segs = chunk_text_chinesev2(input_text, limit=60)

        print(f"ğŸ“ æ–‡æœ¬å·²åˆ†å¥ï¼Œå…± {len(text_segs)} å¥")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # å¤„ç†æ¯ä¸ªå¥å­
        results = []
        for i, sentence in enumerate(text_segs):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(text_segs)} å¥")

            # ç”Ÿæˆæ–‡ä»¶åï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            filename = self.generate_filename(sentence, i+1)
            output_path = os.path.join(output_dir, filename)

            if os.path.exists(output_path) and not force_regenerate:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
                print(f"   ğŸ“ æ–‡æœ¬: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
                results.append({
                    'status': 'skipped',
                    'text': sentence,
                    'output_path': output_path,
                    'filename': filename,
                    'index': i+1
                })
                continue

            result = self.generate_single_audio(
                sentence,
                output_dir,
                index=i+1,
                total_segs=len(text_segs),
                **kwargs
            )
            results.append(result)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(results, output_dir, "ç›´æ¥è¾“å…¥æ–‡æœ¬")

        return results

    def generate_report(self, results, output_dir, text_file):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'source_file': str(text_file),
            'output_directory': str(output_dir),
            'reference_audio': self.reference_wav,
            'device': self.device,
            'summary': {
                'total': len(results),
                'success': len([r for r in results if r['status'] == 'success']),
                'skipped': len([r for r in results if r['status'] == 'skipped']),
                'failed': len([r for r in results if r['status'] == 'failed'])
            },
            'results': results
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'generation_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“Š å¤„ç†æŠ¥å‘Š:")
        print(f"   ğŸ“ æ€»å¥æ•°: {report['summary']['total']}")
        print(f"   âœ… æˆåŠŸ: {report['summary']['success']}")
        print(f"   â­ï¸  è·³è¿‡: {report['summary']['skipped']}")
        print(f"   âŒ å¤±è´¥: {report['summary']['failed']}")
        print(f"   ğŸ“„ æŠ¥å‘Šä¿å­˜: {report_path}")

def merge_audio_files(output_dir, output_filename="merged_audio.wav", gap_ms=500):
    """
    æ ¹æ® generation_report.json åˆå¹¶éŸ³é¢‘æ–‡ä»¶
    åªåˆå¹¶ JSON ä¸­è®°å½•çš„æˆåŠŸç”Ÿæˆçš„æ–‡ä»¶ï¼ŒæŒ‰é¡ºåºåˆå¹¶
    """
    print("ğŸµ å¼€å§‹éŸ³é¢‘åˆå¹¶...")

    # è¯»å– generation_report.json
    report_path = os.path.join(output_dir, 'generation_report.json')
    if not os.path.exists(report_path):
        print(f"âŒ æœªæ‰¾åˆ°ç”ŸæˆæŠ¥å‘Š: {report_path}")
        print("   è¯·å…ˆè¿è¡Œç”Ÿæˆæ­¥éª¤")
        return None

    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
        return None

    # ä»æŠ¥å‘Šä¸­æå–æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    for result in report.get('results', []):
        if result['status'] in ['success', 'skipped'] and result.get('output_path'):
            file_path = result['output_path']
            if os.path.exists(file_path):
                audio_files.append((file_path, result['filename'], result.get('text', '')[:30]))
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {result['filename']}")

    if not audio_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„éŸ³é¢‘æ–‡ä»¶")
        return None

    print(f"ğŸ“ ä»æŠ¥å‘Šä¸­æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

    try:
        from pydub import AudioSegment

        # åŠ è½½ç¬¬ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
        merged = AudioSegment.from_wav(audio_files[0][0])
        print(f"ğŸ”„ [1/{len(audio_files)}] {audio_files[0][1]}")
        print(f"   ğŸ“ {audio_files[0][2]}...")

        # åˆå¹¶å…¶ä»–éŸ³é¢‘æ–‡ä»¶
        for i, (file_path, file_name, text_preview) in enumerate(audio_files[1:], start=2):
            # æ·»åŠ é™éŸ³é—´éš”
            silence = AudioSegment.silent(duration=gap_ms)
            merged += silence

            # æ·»åŠ éŸ³é¢‘æ–‡ä»¶
            audio = AudioSegment.from_wav(file_path)
            merged += audio
            print(f"ğŸ”„ [{i}/{len(audio_files)}] {file_name}")
            print(f"   ğŸ“ {text_preview}...")

        # ä¿å­˜åˆå¹¶åçš„éŸ³é¢‘
        output_path = os.path.join(output_dir, output_filename)
        merged.export(output_path, format="wav")

        # è®¡ç®—æ€»æ—¶é•¿
        duration_seconds = len(merged) / 1000.0
        duration_minutes = duration_seconds / 60.0

        print(f"\nâœ… éŸ³é¢‘åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“ åˆå¹¶æ–‡ä»¶: {output_path}")
        print(f"â±ï¸  æ€»æ—¶é•¿: {duration_minutes:.2f} åˆ†é’Ÿ ({duration_seconds:.1f} ç§’)")
        print(f"ğŸ”‡ é™éŸ³é—´éš”: {gap_ms}ms")
        print(f"ğŸ“Š åˆå¹¶æ–‡ä»¶æ•°: {len(audio_files)}")

        return output_path

    except ImportError:
        print("âŒ ç¼ºå°‘ pydub åº“ï¼Œè¯·å®‰è£…: pip install pydub")
        return None
    except Exception as e:
        print(f"âŒ éŸ³é¢‘åˆå¹¶å¤±è´¥: {e}")
        return None

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¢é‡å¼é€å¥éŸ³é¢‘ç”Ÿæˆå™¨ - æŒ‰å¥å­åˆ†æ®µç”Ÿæˆå’Œåˆå¹¶éŸ³é¢‘')

    parser.add_argument('--input_wav', help='å‚è€ƒéŸ³é¢‘æ–‡ä»¶')
    parser.add_argument('--input_npy', help='å‚è€ƒç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input_text', help='ç›´æ¥è¾“å…¥çš„æ–‡æœ¬å†…å®¹')
    parser.add_argument('--output_dir', default='./output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--time_step', type=int, default=16, help='æ¨ç†æ­¥æ•°')
    parser.add_argument('--p_w', type=float, default=1.6, help='æ¸…æ™°åº¦æƒé‡ (1.0-3.0)')
    parser.add_argument('--t_w', type=float, default=2.5, help='ç›¸ä¼¼åº¦æƒé‡ (2.0-5.0)')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--merge_only', action='store_true', help='ä»…æ‰§è¡ŒéŸ³é¢‘åˆå¹¶ï¼ˆæ ¹æ®generation_report.jsonï¼‰')
    parser.add_argument('--merge_gap', type=int, default=500, help='åˆå¹¶æ—¶çš„é™éŸ³é—´éš”(ms)')

    # è§£æå‚æ•°
    args = parser.parse_args()

    if args.merge_only:
        # ä»…æ‰§è¡ŒéŸ³é¢‘åˆå¹¶
        if not args.output_dir:
            parser.error("--output_dir åœ¨åˆå¹¶æ¨¡å¼ä¸‹æ˜¯å¿…éœ€çš„")

        output_filename = f"merged_audio_{int(time.time())}.wav"
        result = merge_audio_files(args.output_dir, output_filename, args.merge_gap)
        return

    # æ£€æŸ¥å¿…éœ€çš„å‚æ•°
    if not args.input_wav:
        parser.error("--input_wav æ˜¯å¿…éœ€çš„")

    # æ£€æŸ¥æ–‡æœ¬è¾“å…¥æ–¹å¼
    if not args.text_file and not args.input_text:
        parser.error("éœ€è¦æä¾› --text_file æˆ– --input_text å‚æ•°ä¹‹ä¸€")

    if args.text_file and args.input_text:
        parser.error("ä¸èƒ½åŒæ—¶æä¾› --text_file å’Œ --input_text å‚æ•°")

    # ç”Ÿæˆå‚æ•°
    kwargs = {
        'time_step': args.time_step,
        'p_w': args.p_w,
        't_w': args.t_w
    }

    print("="*80)
    print("ğŸš€ å¢é‡å¼é€å¥éŸ³é¢‘ç”Ÿæˆå™¨")
    print("="*80)
    if args.text_file:
        print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶: {args.text_file}")
    else:
        print("ğŸ“ ç›´æ¥è¾“å…¥æ–‡æœ¬")
    print(f"ğŸµ å‚è€ƒéŸ³é¢‘: {args.input_wav}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ”§ æ¨ç†æ­¥æ•°: {args.time_step}")
    print(f"âš™ï¸  æ¸…æ™°åº¦æƒé‡(p_w): {args.p_w}")
    print(f"âš™ï¸  ç›¸ä¼¼åº¦æƒé‡(t_w): {args.t_w}")
    print(f"âš¡ å¼ºåˆ¶é‡ç”Ÿæˆ: {args.force}")
    print("="*80)

    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = IncrementalTTSGenerator(
        reference_wav=args.input_wav,
        reference_npy=args.input_npy
    )

    # å¤„ç†æ–‡æœ¬
    if args.text_file:
        results = generator.process_text_file(
            text_file=args.text_file,
            output_dir=args.output_dir,
            force_regenerate=args.force,
            **kwargs
        )
    else:
        results = generator.process_direct_text(
            input_text=args.input_text,
            output_dir=args.output_dir,
            **kwargs
        )

    # æç¤ºåˆå¹¶å‘½ä»¤
    success_count = sum(1 for r in results if r['status'] == 'success')
    skipped_count = sum(1 for r in results if r['status'] == 'skipped')
    total_audio = success_count + skipped_count

    if total_audio > 0:
        print(f"\nğŸ’¡ æç¤º: å…±æœ‰ {total_audio} ä¸ªéŸ³é¢‘æ–‡ä»¶å¯åˆå¹¶")
        print("   è¿è¡Œä»¥ä¸‹å‘½ä»¤åˆå¹¶éŸ³é¢‘:")
        print(f"   python {__file__} --merge_only --output_dir {args.output_dir}")
        print("   æˆ–ä½¿ç”¨è„šæœ¬:")
        print(f"   ./gen/gen.sh merge")

if __name__ == '__main__':
    main()
