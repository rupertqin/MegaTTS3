#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡å¼é€å¥éŸ³é¢‘ç”Ÿæˆå™¨
æŒ‰å¥å­é€ä¸ªç”ŸæˆéŸ³é¢‘ï¼Œæ”¯æŒè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
"""

import os
import sys
import json
import hashlib
import argparse
import time
from pathlib import Path
from tqdm import tqdm
import wave

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from tts.infer_cli import MegaTTS3DiTInfer
from tts.utils.text_utils.split_text import chunk_text_chinesev2, chunk_text_english
from langdetect import detect as classify_language
import torch

class IncrementalTTSGenerator:
    def __init__(self, reference_wav, reference_npy=None, device=None):
        self.reference_wav = reference_wav
        self.reference_npy = reference_npy or reference_wav.replace('.wav', '.npy')
        self.device = device or ("mps" if torch.backends.mps.is_available() else "cpu")

        # åˆå§‹åŒ–æ¨¡å‹
        print(f"ğŸ”„ åˆå§‹åŒ–æ¨¡å‹ï¼Œè®¾å¤‡: {self.device}")
        self.infer_pipe = MegaTTS3DiTInfer(device=self.device)

        # åŠ è½½å‚è€ƒéŸ³é¢‘
        print("ğŸ”„ åŠ è½½å‚è€ƒéŸ³é¢‘...")
        with open(self.reference_wav, 'rb') as file:
            file_content = file.read()
        self.resource_context = self.infer_pipe.preprocess(file_content, latent_file=self.reference_npy)
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # å¯¼å…¥å¿…è¦çš„å‡½æ•°
        from tts.frontend_function import g2p, dur_pred, prepare_inputs_for_dit
        from tts.utils.audio_utils.io import to_wav_bytes
        from tn.chinese.normalizer import Normalizer as ZhNormalizer
        from tn.english.normalizer import Normalizer as EnNormalizer
        import pyloudnorm as pyln
        import numpy as np

        self.g2p = g2p
        self.dur_pred = dur_pred
        self.prepare_inputs_for_dit = prepare_inputs_for_dit
        self.to_wav_bytes = to_wav_bytes
        self.zh_normalizer = ZhNormalizer()
        self.en_normalizer = EnNormalizer()
        self.pyln = pyln
        self.np = np

    def generate_filename(self, text, index=None):
        """ç”ŸæˆéŸ³é¢‘æ–‡ä»¶åï¼Œä½¿ç”¨å®Œæ•´æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰"""
        import re

        # å»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡
        # ä¿ç•™ç©ºæ ¼ç”¨äºåç»­æ›¿æ¢
        safe_text = re.sub(r'[^\w\s]', '', text)
        # å»é™¤å¤šä½™ç©ºæ ¼
        safe_text = re.sub(r'\s+', '_', safe_text.strip())

        # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œæˆªæ–­ä½†ä¿ç•™å®Œæ•´æ€§
        max_length = 200  # æ–‡ä»¶åæœ€å¤§é•¿åº¦
        if len(safe_text) > max_length:
            safe_text = safe_text[:max_length]

        if index is not None:
            filename = f"{index:03d}_{safe_text}.wav"
        else:
            filename = f"{safe_text}.wav"

        return filename

    def generate_single_sentence(self, text, seg_i, total_segs, time_step, p_w, t_w, dur_disturb=0.1, dur_alpha=1.0):
        """ç”Ÿæˆå•ä¸ªå¥å­çš„éŸ³é¢‘ï¼ˆåº•å±‚å®ç°ï¼Œä¸è°ƒç”¨forwardï¼‰"""
        device = self.device

        # ä» resource_context è·å–å‚è€ƒæ•°æ®
        ph_ref = self.resource_context['ph_ref'].to(device)
        tone_ref = self.resource_context['tone_ref'].to(device)
        mel2ph_ref = self.resource_context['mel2ph_ref'].to(device)
        vae_latent = self.resource_context['vae_latent'].to(device)
        ctx_dur_tokens = self.resource_context['ctx_dur_tokens'].to(device)
        incremental_state_dur_prompt = self.resource_context['incremental_state_dur_prompt']

        with torch.inference_mode():
            # G2P
            ph_pred, tone_pred = self.g2p(self.infer_pipe, text)

            # Duration Prediction
            mel2ph_pred = self.dur_pred(
                self.infer_pipe, ctx_dur_tokens, incremental_state_dur_prompt,
                ph_pred, tone_pred, seg_i, dur_disturb, dur_alpha,
                is_first=(seg_i==0), is_final=(seg_i==total_segs-1)
            )

            # Prepare inputs
            inputs = self.prepare_inputs_for_dit(
                self.infer_pipe, mel2ph_ref, mel2ph_pred,
                ph_ref, tone_ref, ph_pred, tone_pred, vae_latent
            )

            # DiT inference
            with torch.cuda.amp.autocast(dtype=self.infer_pipe.precision, enabled=True):
                x = self.infer_pipe.dit.inference(inputs, timesteps=time_step, seq_cfg_w=[p_w, t_w]).float()

            # WavVAE decode
            x[:, :vae_latent.size(1)] = vae_latent
            wav_pred = self.infer_pipe.wavvae.decode(x)[0,0].to(torch.float32)

            # Post-processing
            wav_pred = wav_pred[vae_latent.size(1)*self.infer_pipe.vae_stride*self.infer_pipe.hop_size:].cpu().numpy()

            # Normalize loudness
            meter = self.pyln.Meter(self.infer_pipe.sr)
            loudness_pred = self.infer_pipe.loudness_meter.integrated_loudness(wav_pred.astype(float))
            wav_pred = self.pyln.normalize.loudness(wav_pred, loudness_pred, self.infer_pipe.loudness_prompt)
            if self.np.abs(wav_pred).max() >= 1:
                wav_pred = wav_pred / self.np.abs(wav_pred).max() * 0.95

            return wav_pred

    def generate_single_audio(self, text, output_dir, index=None, total_segs=1, **kwargs):
        """ç”Ÿæˆå•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å
        filename = self.generate_filename(text, index)
        output_path = os.path.join(output_dir, filename)

        try:
            print(f"   ğŸ“ æ–‡æœ¬: {text[:50]}{'...' if len(text) > 50 else ''}")

            # ç”ŸæˆéŸ³é¢‘
            start_time = time.time()
            seg_i = (index - 1) if index else 0
            wav_pred = self.generate_single_sentence(
                text, seg_i, total_segs,
                kwargs.get('time_step', 16),
                kwargs.get('p_w', 1.6),
                kwargs.get('t_w', 2.5),
                kwargs.get('dur_disturb', 0.1),
                kwargs.get('dur_alpha', 1.0)
            )

            # è½¬æ¢ä¸º wav bytes å¹¶ä¿å­˜
            wav_bytes = self.to_wav_bytes(wav_pred, self.infer_pipe.sr)
            from tts.utils.audio_utils.io import save_wav
            save_wav(wav_bytes, output_path)

            generation_time = time.time() - start_time

            print(f"   âœ… å®Œæˆ - è€—æ—¶: {generation_time:.1f}ç§’")

            # è·å–éŸ³é¢‘æ—¶é•¿
            duration = self.get_audio_duration(output_path)

            return {
                'status': 'success',
                'text': text,
                'output_path': output_path,
                'filename': filename,
                'index': index,
                'generation_time': generation_time,
                'duration': duration
            }

        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'status': 'failed',
                'text': text,
                'output_path': None,
                'filename': filename,
                'index': index,
                'error': str(e)
            }

    def get_audio_duration(self, audio_path):
        """è·å–éŸ³é¢‘æ–‡ä»¶æ—¶é•¿ï¼ˆç§’ï¼‰"""
        try:
            with wave.open(audio_path, 'rb') as audio_file:
                frames = audio_file.getnframes()
                rate = audio_file.getframerate()
                duration = frames / float(rate)
                return duration
        except Exception as e:
            print(f"âš ï¸  è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
            return 0.0

    def normalize_text_for_matching(self, text):
        """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºåŒ¹é…ï¼šå»é™¤æ‰€æœ‰æ ‡ç‚¹å’Œç©ºæ ¼"""
        import re
        return re.sub(r'[^\w]', '', text)

    def load_existing_report(self, output_dir):
        """åŠ è½½å·²æœ‰çš„ç”ŸæˆæŠ¥å‘Š"""
        report_path = os.path.join(output_dir, 'generation_report.json')
        if os.path.exists(report_path):
            try:
                with open(report_path, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                return report
            except Exception as e:
                print(f"âš ï¸  è¯»å–æŠ¥å‘Šå¤±è´¥: {e}")
        return None

    def build_existing_sentences_map(self, existing_report):
        """æ„å»ºå·²å¤„ç†å¥å­çš„æ˜ å°„è¡¨ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ–‡æœ¬ä½œä¸ºkeyï¼‰"""
        existing_sentences = {}
        if existing_report:
            for result in existing_report.get('results', []):
                if result['status'] in ['success', 'skipped']:
                    # æ ‡å‡†åŒ–æ–‡æœ¬ï¼šå»é™¤æ‰€æœ‰æ ‡ç‚¹å’Œç©ºæ ¼ï¼Œç”¨äºåŒ¹é…
                    normalized_text = self.normalize_text_for_matching(result['text'])
                    existing_sentences[normalized_text] = result
        return existing_sentences

    def process_text_file(self, text_file, output_dir, force_regenerate=False, **kwargs):
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘"""
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        with open(text_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()

        # æ£€æµ‹è¯­è¨€å¹¶åˆ†å¥
        language_type = classify_language(content)

        if language_type == 'en':
            print("ğŸŒ æ£€æµ‹åˆ°è‹±æ–‡æ–‡æœ¬")
            content = self.en_normalizer.normalize(content)
            text_segs = chunk_text_english(content, max_chars=130)
        else:
            print("ğŸŒ æ£€æµ‹åˆ°ä¸­æ–‡æ–‡æœ¬")
            content = self.zh_normalizer.normalize(content)
            text_segs = chunk_text_chinesev2(content, limit=60)

        print(f"ğŸ“ æ–‡æœ¬å·²åˆ†å¥ï¼Œå…± {len(text_segs)} å¥")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # å¤„ç†æ¯ä¸ªå¥å­
        results = []
        for i, sentence in enumerate(text_segs):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(text_segs)} å¥")

            # ç”Ÿæˆæ–‡ä»¶åï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            filename = self.generate_filename(sentence, i+1)
            output_path = os.path.join(output_dir, filename)

            if os.path.exists(output_path) and not force_regenerate:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
                print(f"   ğŸ“ æ–‡æœ¬: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
                # è·å–å·²å­˜åœ¨æ–‡ä»¶çš„æ—¶é•¿
                duration = self.get_audio_duration(output_path)
                results.append({
                    'status': 'skipped',
                    'text': sentence,
                    'output_path': output_path,
                    'filename': filename,
                    'index': i+1,
                    'duration': duration
                })
                continue

            result = self.generate_single_audio(
                sentence,
                output_dir,
                index=i+1,
                total_segs=len(text_segs),
                **kwargs
            )
            results.append(result)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(results, output_dir, text_file)

        return results

    def process_direct_text(self, input_text, output_dir, force_regenerate=False, **kwargs):
        """å¤„ç†ç›´æ¥è¾“å…¥çš„æ–‡æœ¬ï¼Œé€å¥ç”ŸæˆéŸ³é¢‘"""
        # æ£€æµ‹è¯­è¨€å¹¶åˆ†å¥
        language_type = classify_language(input_text)

        if language_type == 'en':
            print("ğŸŒ æ£€æµ‹åˆ°è‹±æ–‡æ–‡æœ¬")
            input_text = self.en_normalizer.normalize(input_text)
            text_segs = chunk_text_english(input_text, max_chars=130)
        else:
            print("ğŸŒ æ£€æµ‹åˆ°ä¸­æ–‡æ–‡æœ¬")
            input_text = self.zh_normalizer.normalize(input_text)
            text_segs = chunk_text_chinesev2(input_text, limit=60)

        print(f"ğŸ“ æ–‡æœ¬å·²åˆ†å¥ï¼Œå…± {len(text_segs)} å¥")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # å¤„ç†æ¯ä¸ªå¥å­
        results = []
        for i, sentence in enumerate(text_segs):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(text_segs)} å¥")

            # ç”Ÿæˆæ–‡ä»¶åï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            filename = self.generate_filename(sentence, i+1)
            output_path = os.path.join(output_dir, filename)

            if os.path.exists(output_path) and not force_regenerate:
                print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
                print(f"   ğŸ“ æ–‡æœ¬: {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
                # è·å–å·²å­˜åœ¨æ–‡ä»¶çš„æ—¶é•¿
                duration = self.get_audio_duration(output_path)
                results.append({
                    'status': 'skipped',
                    'text': sentence,
                    'output_path': output_path,
                    'filename': filename,
                    'index': i+1,
                    'duration': duration
                })
                continue

            result = self.generate_single_audio(
                sentence,
                output_dir,
                index=i+1,
                total_segs=len(text_segs),
                **kwargs
            )
            results.append(result)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(results, output_dir, "ç›´æ¥è¾“å…¥æ–‡æœ¬")

        return results

    def generate_report(self, results, output_dir, text_file):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'source_file': str(text_file),
            'output_directory': str(output_dir),
            'reference_audio': self.reference_wav,
            'device': self.device,
            'summary': {
                'total': len(results),
                'success': len([r for r in results if r['status'] == 'success']),
                'skipped': len([r for r in results if r['status'] == 'skipped']),
                'failed': len([r for r in results if r['status'] == 'failed'])
            },
            'results': results
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'generation_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“Š å¤„ç†æŠ¥å‘Š:")
        print(f"   ğŸ“ æ€»å¥æ•°: {report['summary']['total']}")
        print(f"   âœ… æˆåŠŸ: {report['summary']['success']}")
        print(f"   â­ï¸  è·³è¿‡: {report['summary']['skipped']}")
        print(f"   âŒ å¤±è´¥: {report['summary']['failed']}")
        print(f"   ğŸ“„ æŠ¥å‘Šä¿å­˜: {report_path}")

def generate_srt_files(output_dir, gap_ms=500):
    """
    æ ¹æ® generation_report.json ç”Ÿæˆ SRT å­—å¹•æ–‡ä»¶
    ä¸ºæ¯ä¸ªéŸ³é¢‘ç‰‡æ®µç”Ÿæˆç‹¬ç«‹çš„ SRT æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆåˆå¹¶çš„ SRT æ–‡ä»¶
    """
    print("ğŸ“ å¼€å§‹ç”Ÿæˆ SRT å­—å¹•æ–‡ä»¶...")

    # è¯»å– generation_report.json
    report_path = os.path.join(output_dir, 'generation_report.json')
    if not os.path.exists(report_path):
        print(f"âŒ æœªæ‰¾åˆ°ç”ŸæˆæŠ¥å‘Š: {report_path}")
        print("   è¯·å…ˆè¿è¡Œç”Ÿæˆæ­¥éª¤")
        return None

    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
        return None

    # ä»æŠ¥å‘Šä¸­æå–æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
    subtitle_entries = []
    for result in report.get('results', []):
        if result['status'] in ['success', 'skipped'] and result.get('output_path'):
            file_path = result['output_path']
            if os.path.exists(file_path):
                # ä¼˜å…ˆä½¿ç”¨æŠ¥å‘Šä¸­çš„ durationï¼Œå¦‚æœæ²¡æœ‰åˆ™åŠ¨æ€è®¡ç®—
                duration = result.get('duration')
                if duration is None or duration == 0.0:
                    # åŠ¨æ€è®¡ç®—éŸ³é¢‘æ—¶é•¿
                    try:
                        with wave.open(file_path, 'rb') as audio_file:
                            frames = audio_file.getnframes()
                            rate = audio_file.getframerate()
                            duration = frames / float(rate)
                    except Exception as e:
                        print(f"âš ï¸  æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿: {result['filename']}, é”™è¯¯: {e}")
                        duration = 3.0  # é»˜è®¤ 3 ç§’

                subtitle_entries.append({
                    'index': result['index'],
                    'text': result['text'],
                    'filename': result['filename'],
                    'duration': duration
                })
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {result['filename']}")

    if not subtitle_entries:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”Ÿæˆå­—å¹•çš„éŸ³é¢‘æ–‡ä»¶")
        return None

    print(f"ğŸ“ ä»æŠ¥å‘Šä¸­æ‰¾åˆ° {len(subtitle_entries)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

    # ç”Ÿæˆå•ç‹¬çš„ SRT æ–‡ä»¶
    for entry in subtitle_entries:
        srt_filename = entry['filename'].replace('.wav', '.srt')
        srt_path = os.path.join(output_dir, srt_filename)

        # å°†æ–‡æœ¬åˆ†å‰²æˆå¤šè¡Œï¼ˆæ¯è¡Œä¸è¶…è¿‡15ä¸ªå­—ï¼‰
        subtitle_lines = split_subtitle_text(entry['text'], max_chars=15)

        # è®¡ç®—æ¯è¡Œçš„æ—¶é•¿ï¼ˆå¹³å‡åˆ†é…ï¼‰
        total_duration = entry['duration']
        line_duration = total_duration / len(subtitle_lines) if subtitle_lines else total_duration

        # ç”Ÿæˆ SRT å†…å®¹
        srt_content = ""
        for i, line in enumerate(subtitle_lines, start=1):
            start_time = format_srt_time((i - 1) * line_duration)
            end_time = format_srt_time(i * line_duration)
            srt_content += f"{i}\n{start_time} --> {end_time}\n{line}\n\n"

        with open(srt_path, 'w', encoding='utf-8') as f:
            f.write(srt_content)

        print(f"âœ… ç”Ÿæˆå­—å¹•: {srt_filename} ({len(subtitle_lines)} è¡Œ)")

    # ç”Ÿæˆåˆå¹¶çš„ SRT æ–‡ä»¶
    merged_srt_path = os.path.join(output_dir, "merged_subtitles.srt")
    gap_seconds = gap_ms / 1000.0
    current_time = 0.0
    subtitle_index = 1

    with open(merged_srt_path, 'w', encoding='utf-8') as f:
        for entry in subtitle_entries:
            # å°†æ–‡æœ¬åˆ†å‰²æˆå¤šè¡Œï¼ˆæ¯è¡Œä¸è¶…è¿‡15ä¸ªå­—ï¼‰
            subtitle_lines = split_subtitle_text(entry['text'], max_chars=15)

            # è®¡ç®—æ¯è¡Œçš„æ—¶é•¿ï¼ˆå¹³å‡åˆ†é…ï¼‰
            total_duration = entry['duration']
            line_duration = total_duration / len(subtitle_lines) if subtitle_lines else total_duration

            # ä¸ºæ¯ä¸€è¡Œç”Ÿæˆå­—å¹•æ¡ç›®
            for line in subtitle_lines:
                start_time = format_srt_time(current_time)
                end_time = format_srt_time(current_time + line_duration)

                f.write(f"{subtitle_index}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{line}\n\n")

                subtitle_index += 1
                current_time += line_duration

            # æ·»åŠ é—´éš”æ—¶é—´ï¼ˆåœ¨å¥å­ä¹‹é—´ï¼‰
            current_time += gap_seconds

    print(f"\nâœ… SRT å­—å¹•ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ åˆå¹¶å­—å¹•æ–‡ä»¶: {merged_srt_path}")
    print(f"ğŸ“Š éŸ³é¢‘ç‰‡æ®µæ•°: {len(subtitle_entries)}")
    print(f"ğŸ“Š å­—å¹•æ¡ç›®æ•°: {subtitle_index - 1}")

    return merged_srt_path

def count_chars_without_punctuation(text):
    """è®¡ç®—æ–‡æœ¬å­—ç¬¦æ•°ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·"""
    import re
    clean_text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€,\.!?;:"""''ã€Œã€ã€ã€ï¼ˆï¼‰\(\)\s]', '', text)
    return len(clean_text)

def split_subtitle_text(text, max_chars=15, min_chars=10):
    """
    å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå­—å¹•è¡Œ
    - æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼ˆä¸åŒ…æ‹¬å¼•å·ï¼‰
    - æ¯è¡Œä¸è¶…è¿‡ max_chars ä¸ªå­—ç¬¦ï¼ˆä¸è®¡æ ‡ç‚¹ï¼‰
    - ä¼˜å…ˆåœ¨"çš„"ã€"åœ°"ã€"å¾—"ç­‰åŠ©è¯å¤„æ–­å¼€
    - å¼•å·ä¸è®¡å…¥å­—ç¬¦æ•°
    """
    import re

    # åªæŒ‰è¿™äº›æ ‡ç‚¹åˆ†å‰²ï¼Œä¸åŒ…æ‹¬å¼•å·
    segments = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€,\.!?;:]', text)

    # å»é™¤ç©ºç™½ç‰‡æ®µ
    segments = [s.strip() for s in segments if s.strip()]

    if not segments:
        return []

    # ç¬¬ä¸€æ­¥ï¼šåˆå¹¶è¿‡çŸ­çš„ç‰‡æ®µ
    merged = []
    i = 0
    while i < len(segments):
        current = segments[i]
        current_len = count_chars_without_punctuation(current)

        while current_len < min_chars and i + 1 < len(segments):
            next_text = segments[i + 1]
            combined = current + next_text
            combined_len = count_chars_without_punctuation(combined)

            if combined_len <= max_chars:
                i += 1
                current = combined
                current_len = combined_len
            else:
                break

        merged.append(current)
        i += 1

    # ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½åˆ†å‰²è¶…é•¿ç‰‡æ®µ
    def find_best_split_point(text_part, target_len, min_len):
        """æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹"""
        priority_chars = ['çš„', 'åœ°', 'å¾—', 'äº†', 'ç€', 'è¿‡']
        secondary_chars = ['æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'æŠŠ', 'è¢«']

        # è®¡ç®—æ¯ä¸ªä½ç½®çš„å®é™…å­—ç¬¦æ•°
        positions = []
        count = 0
        for c in text_part:
            if not re.match(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€,\.!?;:"""''ã€Œã€ã€ã€ï¼ˆï¼‰\(\)\s]', c):
                count += 1
            positions.append(count)

        best_pos = -1
        best_score = -1000

        for i in range(len(text_part)):
            curr_len = positions[i]
            remain_len = count - curr_len

            if curr_len < min_len or curr_len > target_len:
                continue

            score = 0

            # ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯ä¼˜å…ˆå­—ç¬¦ï¼ŒåŠ åˆ†
            if i + 1 < len(text_part) and text_part[i + 1] in priority_chars:
                score += 15
            elif i + 1 < len(text_part) and text_part[i + 1] in secondary_chars:
                score += 8

            # è¶Šæ¥è¿‘ç›®æ ‡é•¿åº¦è¶Šå¥½
            score -= abs(curr_len - target_len)

            # å‰©ä½™éƒ¨åˆ†çš„é•¿åº¦è¯„åˆ†
            if remain_len >= min_len:
                score += 5
            elif remain_len > 0:
                score -= (min_len - remain_len) * 3

            if score > best_score:
                best_score = score
                best_pos = i + 1

        return best_pos if best_pos > 0 else min(target_len, len(text_part))

    split_lines = []
    for part in merged:
        part_len = count_chars_without_punctuation(part)

        if part_len <= max_chars:
            split_lines.append(part)
        else:
            # è¶…é•¿ç‰‡æ®µéœ€è¦æ™ºèƒ½åˆ†å‰²
            remaining = part

            while remaining:
                remaining_len = count_chars_without_punctuation(remaining)

                if remaining_len <= max_chars:
                    split_lines.append(remaining)
                    break

                # æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹
                best_pos = find_best_split_point(remaining, max_chars, min_chars)

                if best_pos > 0 and best_pos < len(remaining):
                    split_lines.append(remaining[:best_pos].strip())
                    remaining = remaining[best_pos:].strip()
                else:
                    split_lines.append(remaining)
                    break

    # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶è¿‡çŸ­çš„ç‰‡æ®µ
    final_lines = []
    for part in split_lines:
        part_len = count_chars_without_punctuation(part)

        if part_len < min_chars and final_lines:
            prev_len = count_chars_without_punctuation(final_lines[-1])
            combined_len = prev_len + part_len

            if combined_len <= max_chars:
                final_lines[-1] = final_lines[-1] + part
            else:
                final_lines.append(part)
        else:
            final_lines.append(part)

    return final_lines


def format_srt_time(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸º SRT æ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

def merge_audio_files(output_dir, output_filename="merged_audio.wav", gap_ms=500):
    """
    æ ¹æ® generation_report.json åˆå¹¶éŸ³é¢‘æ–‡ä»¶
    åªåˆå¹¶ JSON ä¸­è®°å½•çš„æˆåŠŸç”Ÿæˆçš„æ–‡ä»¶ï¼ŒæŒ‰é¡ºåºåˆå¹¶
    """
    print("ğŸµ å¼€å§‹éŸ³é¢‘åˆå¹¶...")

    # è¯»å– generation_report.json
    report_path = os.path.join(output_dir, 'generation_report.json')
    if not os.path.exists(report_path):
        print(f"âŒ æœªæ‰¾åˆ°ç”ŸæˆæŠ¥å‘Š: {report_path}")
        print("   è¯·å…ˆè¿è¡Œç”Ÿæˆæ­¥éª¤")
        return None

    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
        return None

    # ä»æŠ¥å‘Šä¸­æå–æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    for result in report.get('results', []):
        if result['status'] in ['success', 'skipped'] and result.get('output_path'):
            file_path = result['output_path']
            if os.path.exists(file_path):
                audio_files.append((file_path, result['filename'], result.get('text', '')[:30]))
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {result['filename']}")

    if not audio_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„éŸ³é¢‘æ–‡ä»¶")
        return None

    print(f"ğŸ“ ä»æŠ¥å‘Šä¸­æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

    try:
        from pydub import AudioSegment

        # åŠ è½½ç¬¬ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶
        merged = AudioSegment.from_wav(audio_files[0][0])
        print(f"ğŸ”„ [1/{len(audio_files)}] {audio_files[0][1]}")
        print(f"   ğŸ“ {audio_files[0][2]}...")

        # åˆå¹¶å…¶ä»–éŸ³é¢‘æ–‡ä»¶
        for i, (file_path, file_name, text_preview) in enumerate(audio_files[1:], start=2):
            # æ·»åŠ é™éŸ³é—´éš”
            silence = AudioSegment.silent(duration=gap_ms)
            merged += silence

            # æ·»åŠ éŸ³é¢‘æ–‡ä»¶
            audio = AudioSegment.from_wav(file_path)
            merged += audio
            print(f"ğŸ”„ [{i}/{len(audio_files)}] {file_name}")
            print(f"   ğŸ“ {text_preview}...")

        # ä¿å­˜åˆå¹¶åçš„éŸ³é¢‘
        output_path = os.path.join(output_dir, output_filename)
        merged.export(output_path, format="wav")

        # è®¡ç®—æ€»æ—¶é•¿
        duration_seconds = len(merged) / 1000.0
        duration_minutes = duration_seconds / 60.0

        print(f"\nâœ… éŸ³é¢‘åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“ åˆå¹¶æ–‡ä»¶: {output_path}")
        print(f"â±ï¸  æ€»æ—¶é•¿: {duration_minutes:.2f} åˆ†é’Ÿ ({duration_seconds:.1f} ç§’)")
        print(f"ğŸ”‡ é™éŸ³é—´éš”: {gap_ms}ms")
        print(f"ğŸ“Š åˆå¹¶æ–‡ä»¶æ•°: {len(audio_files)}")

        return output_path

    except ImportError:
        print("âŒ ç¼ºå°‘ pydub åº“ï¼Œè¯·å®‰è£…: pip install pydub")
        return None
    except Exception as e:
        print(f"âŒ éŸ³é¢‘åˆå¹¶å¤±è´¥: {e}")
        return None

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¢é‡å¼é€å¥éŸ³é¢‘ç”Ÿæˆå™¨ - æŒ‰å¥å­åˆ†æ®µç”Ÿæˆå’Œåˆå¹¶éŸ³é¢‘')

    parser.add_argument('--input_wav', help='å‚è€ƒéŸ³é¢‘æ–‡ä»¶')
    parser.add_argument('--input_npy', help='å‚è€ƒç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--text_file', help='æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input_text', help='ç›´æ¥è¾“å…¥çš„æ–‡æœ¬å†…å®¹')
    parser.add_argument('--output_dir', default='./output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--time_step', type=int, default=16, help='æ¨ç†æ­¥æ•°')
    parser.add_argument('--p_w', type=float, default=1.6, help='æ¸…æ™°åº¦æƒé‡ (1.0-3.0)')
    parser.add_argument('--t_w', type=float, default=2.5, help='ç›¸ä¼¼åº¦æƒé‡ (2.0-5.0)')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--merge_only', action='store_true', help='ä»…æ‰§è¡ŒéŸ³é¢‘åˆå¹¶ï¼ˆæ ¹æ®generation_report.jsonï¼‰')
    parser.add_argument('--merge_gap', type=int, default=500, help='åˆå¹¶æ—¶çš„é™éŸ³é—´éš”(ms)')
    parser.add_argument('--srt_only', action='store_true', help='ä»…ç”Ÿæˆ SRT å­—å¹•æ–‡ä»¶ï¼ˆæ ¹æ®generation_report.jsonï¼‰')

    # è§£æå‚æ•°
    args = parser.parse_args()

    if args.srt_only:
        # ä»…ç”Ÿæˆ SRT å­—å¹•
        if not args.output_dir:
            parser.error("--output_dir åœ¨ SRT ç”Ÿæˆæ¨¡å¼ä¸‹æ˜¯å¿…éœ€çš„")

        result = generate_srt_files(args.output_dir, args.merge_gap)
        return

    if args.merge_only:
        # ä»…æ‰§è¡ŒéŸ³é¢‘åˆå¹¶
        if not args.output_dir:
            parser.error("--output_dir åœ¨åˆå¹¶æ¨¡å¼ä¸‹æ˜¯å¿…éœ€çš„")

        output_filename = f"merged_audio_{int(time.time())}.wav"
        audio_result = merge_audio_files(args.output_dir, output_filename, args.merge_gap)

        # åŒæ—¶ç”Ÿæˆ SRT å­—å¹•
        print("\n" + "="*80)
        srt_result = generate_srt_files(args.output_dir, args.merge_gap)
        return

    # æ£€æŸ¥å¿…éœ€çš„å‚æ•°
    if not args.input_wav:
        parser.error("--input_wav æ˜¯å¿…éœ€çš„")

    # æ£€æŸ¥æ–‡æœ¬è¾“å…¥æ–¹å¼
    if not args.text_file and not args.input_text:
        parser.error("éœ€è¦æä¾› --text_file æˆ– --input_text å‚æ•°ä¹‹ä¸€")

    if args.text_file and args.input_text:
        parser.error("ä¸èƒ½åŒæ—¶æä¾› --text_file å’Œ --input_text å‚æ•°")

    # ç”Ÿæˆå‚æ•°
    kwargs = {
        'time_step': args.time_step,
        'p_w': args.p_w,
        't_w': args.t_w
    }

    print("="*80)
    print("ğŸš€ å¢é‡å¼é€å¥éŸ³é¢‘ç”Ÿæˆå™¨")
    print("="*80)
    if args.text_file:
        print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶: {args.text_file}")
    else:
        print("ğŸ“ ç›´æ¥è¾“å…¥æ–‡æœ¬")
    print(f"ğŸµ å‚è€ƒéŸ³é¢‘: {args.input_wav}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ”§ æ¨ç†æ­¥æ•°: {args.time_step}")
    print(f"âš™ï¸  æ¸…æ™°åº¦æƒé‡(p_w): {args.p_w}")
    print(f"âš™ï¸  ç›¸ä¼¼åº¦æƒé‡(t_w): {args.t_w}")
    print(f"âš¡ å¼ºåˆ¶é‡ç”Ÿæˆ: {args.force}")
    print("="*80)

    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = IncrementalTTSGenerator(
        reference_wav=args.input_wav,
        reference_npy=args.input_npy
    )

    # å¤„ç†æ–‡æœ¬
    if args.text_file:
        results = generator.process_text_file(
            text_file=args.text_file,
            output_dir=args.output_dir,
            force_regenerate=args.force,
            **kwargs
        )
    else:
        results = generator.process_direct_text(
            input_text=args.input_text,
            output_dir=args.output_dir,
            **kwargs
        )

    # æç¤ºåˆå¹¶å‘½ä»¤
    success_count = sum(1 for r in results if r['status'] == 'success')
    skipped_count = sum(1 for r in results if r['status'] == 'skipped')
    total_audio = success_count + skipped_count

    if total_audio > 0:
        print(f"\nğŸ’¡ æç¤º: å…±æœ‰ {total_audio} ä¸ªéŸ³é¢‘æ–‡ä»¶å¯åˆå¹¶")
        print("   è¿è¡Œä»¥ä¸‹å‘½ä»¤åˆå¹¶éŸ³é¢‘å’Œç”Ÿæˆå­—å¹•:")
        print(f"   python {__file__} --merge_only --output_dir {args.output_dir}")
        print("   æˆ–ä½¿ç”¨è„šæœ¬:")
        print(f"   ./gen/gen.sh merge")
        print("\n   ä»…ç”Ÿæˆ SRT å­—å¹•:")
        print(f"   python {__file__} --srt_only --output_dir {args.output_dir}")

if __name__ == '__main__':
    main()
